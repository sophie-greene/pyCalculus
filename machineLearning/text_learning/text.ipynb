{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sw=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import clear_output, display, display_pretty\n",
    "len(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "def parseOutText(f):\n",
    "    \"\"\" given an opened email file f, parse out all text below the\n",
    "        metadata block at the top\n",
    "        (in Part 2, you will also add stemming capabilities)\n",
    "        and return a string that contains all the words\n",
    "        in the email (space-separated) \n",
    "        \n",
    "        example use case:\n",
    "        f = open(\"email_file_name.txt\", \"r\")\n",
    "        text = parseOutText(f)\n",
    "        \n",
    "        \"\"\"\n",
    "    stemmer=SnowballStemmer('english')\n",
    "    #word_tokenize=tokenize('english')\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "        #text_string=''.join(i for i in text_string if not i.isdigit())\n",
    "\n",
    "#for word in word_tokenize(text_string):\n",
    "        for word in text_string.split():\n",
    "            words = words+\" \"+(stemmer.stem(word))\n",
    "        #print tokens\n",
    "        ### project part 2: comment out the line below\n",
    "        #words = text_string\n",
    "\n",
    "        ### split the text string into individual words, stem each word,\n",
    "        ### and append the stemmed word to words (make sure there's a single\n",
    "        ### space between each stemmed word)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print words\n",
    "       \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " jtowns nonprivilegedpst i just know you are go to call that ladi that andi was talk about today\n",
      " jtowns nonprivilegedpst we have a discount on the ict contract 18793 from 1112001 through 3312002 the receipt point are trcon leidi and trcocga young women creek the deliveri point is trcocng leidi the rate is 025\n",
      " jtowns nonprivilegedpst we are buy the follow tenn transport from the pipelin for the winter i just did the deal so i dont have the contract number term demand comm mdq nov 18 02 3893 novjan 20 02 2500 febmar 20 02 2500 the primari deliveri point is cgas broad run the primari receipt point is meter 010698 in 500l we will be abl to use ani zone 1 receipt point but we onli be abl to use broad run as our deliveri point\n",
      " jtowns nonprivilegedpst i have a discount on k 2891 of 18 from 500l or 800l to midwestern for the 2nd thru 31st my rate on this path was 17 for the 1st\n",
      " judi townsend 62602pst i ask victor about the file i think keep it on your h drive is still a good idea for now origin messag from lamadrid victor sent thursday januari 31 2002 1019 am to   cc townsend judi subject fw move all your file for new compani origin messag from lamadrid victor sent thursday decemb 13 2001 411 pm to storey geoff cc superti robert kinsey lisa sullivan patti gay randal l subject fw move all your file for new compani the directori weeast desk logist would like movedcopi for the new compani is ocommonlogisticsgaseastdesk thank origin messag from lamadrid victor sent thursday decemb 13 2001 226 pm to allwein robert driscollernest mard l garcia clarissa goodel scott halstead lia homco meredith love scott ordway  sanchez tina superti robert versen victoria subject fw move all your file for new compani anoth thought is to simpli copi all of your file onto the new directori if you prefer to do it that way thank origin messag from lamadrid victor sent thursday decemb 13 2001 1127 am to allwein robert driscollernest mard l garcia clarissa goodel scott halstead lia homco meredith love scott ordway  sanchez tina superti robert versen victoria cc   subject move all your file for new compani team pleas move all the file you need to do your job phone list id spreadsheet anyth to ocommonlogisticsgaseastdesk directori a copi of this will be made to conduct busi in new companythank\n",
      " judi townsend 62602pst photo of my woman on the net origin messag from trogg522aolcomenron sent tuesday januari 15 2002 1112 pm to ingridimmerwilliamscom cc   subject anoth photo spot this place take a littl more work to get to but you can add a lot more photoshp photo page 1 go to the link shown abov 2 scroll down until you see a green box that has new to hp photo in bold white letter 3 type in troggsplac in the box beneath view a friend album 4 click on the mother of my child 5 you should see a smaller photo of you at he fountain scroll down and click on look at my album\n",
      " jtownsensf on the 8th tp2 sold vng 10000 dth at st 65 this volum was enter on deal 760980 which is a vng buy from gulf 1 i took the volum on this deal to 0 for the 8th and creat deal 780794 vng buy from tp2 at st 65 pleas make a note in you file meredith you will need to repath these deal on the 8th thank\n",
      " jtownsensf httpgasmsgboardcorpenroncom\n",
      "emails processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from_sara  = open(\"from_sara.txt\", \"r\")\n",
    "from_chris = open(\"from_chris.txt\", \"r\")\n",
    "\n",
    "from_data = []\n",
    "word_data = []\n",
    "\n",
    "### temp_counter is a way to speed up the development--there are\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "### can take a long time\n",
    "### temp_counter helps you only look at the first 200 emails in the list so you\n",
    "### can iterate your modifications quicker\n",
    "temp_counter = 0\n",
    "\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        ### only look at first 200 emails when developing\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "        temp_counter+=1\n",
    "        path = os.path.join('/home/some/ud120-projects', path[:-1])\n",
    "        \n",
    "        email = open(path, \"r\")\n",
    "        sig=['sara', 'shackleton', 'chris', 'germani']\n",
    "        res=parseOutText(email)\n",
    "        for s in sig:\n",
    "            res=res.replace(s,\"\")\n",
    "        print res\n",
    "        \n",
    "        word_data.append(res)\n",
    "        if name=='sara':\n",
    "            value=0\n",
    "        else:\n",
    "            value=1\n",
    "        from_data.append(value)\n",
    "        ### use parseOutText to extract the text from the opened email\n",
    "\n",
    "        ### use str.replace() to remove any instances of the words\n",
    "        ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "\n",
    "        ### append the text to word_data\n",
    "\n",
    "        ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "\n",
    "        if temp_counter==10:\n",
    "            clear_output()\n",
    "            temp_counter=0\n",
    "            #break\n",
    "        #print path\n",
    "        email.close()\n",
    "        \n",
    "print \"emails processed\"\n",
    "from_sara.close()\n",
    "from_chris.close()\n",
    "\n",
    "pickle.dump( word_data, open(\"/home/some/ud120-projects/your_word_data.pkl\", \"w\") )\n",
    "pickle.dump( from_data, open(\"/home/some/ud120-projects/your_email_authors.pkl\", \"w\") )\n",
    "#clear_output()#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38757\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "vectorizer.fit_transform(word_data)\n",
    "\n",
    "vocab_list = vectorizer.get_feature_names()\n",
    "print len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stephaniethank\n"
     ]
    }
   ],
   "source": [
    "print vocab_list[34597]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
